---
layout: post
title: Tech Debt Thoughts
tags: Coder Tester
---

Often when people talk about quality practices, they get bogged down by trying to find the right balance between "getting work done" and "going slow enough to ensure quality". I honestly believe that the quality of code does not have to be in inverse proportion to the speed at which it is written. I think everyone has a good idea (or at least has access to good ideas) of what kinds of tests to write to validate one's code and while the ills of technical debt are well known, very few people talk about how to address it. So let's address the elephant in the room, shall we?

## Definition

Technical debt (or tech debt, code debt or design debt) is a term that refers to all those shortcuts people take when trying to get code done fast. All those times when one says, "After we ship, I'll come back and fix that," or "I know that there is a better way, but it'll take too long," not to mention all those `TODO` or `HACK` tags one leaves in the code. Most often though, "I'll get back to that" never comes. Sometimes it is good intentions paving the way to the Hell of Fixing One Bug Creates Two More, sometimes it is because management is too date-driven and sometimes it is because nobody wants to clean up the mess.

But let's assume that someone finally woke up and decided that the code needs an extreme makeover. How do we do that?

## Tenets

These are my tenets for turning a code base around:

* Create tools to educate, encourage and, if necessary, enforce quality practices
* Design systems to create a [Pit of Success][pit-of-success]
* Understand that different people work differently and create quality systems that enable people to provide value according to their strengths rather than focusing on their weaknesses
* Only add process if it is absolutely necessary
* When process is necessary, keep it as lightweight as possible

## Haters Gonna Hate, Builders Gonna Build

So get out of the way of people who want to build by adding automated quality checks and tools to the developer's process:

* Edit - Style checks, Static tests, Auto-compile, Tools to make writing code documentation easier
* Build - Style checks, static tests, documentation generation, unit tests
* Code Review - Create a tool to add results of style checks, static tests, unit tests to the code review description
* Commit - Prevent direct commit to mainline, all work is done on feature branches
* Merge - Prevent merging to mainline if tests or tools fail

There are multiple layers here:

1. Tools that educate and inform as the engineer writes the code
1. Tools that encourage better practices throughout the process
1. Tools that enforce a minimum standard before sharing code with other developers

## Path to Green

### Clean Up As You Go

When I first moved out, like most American young adults, I had a problem with keeping my place clean. Then I got some great advice from people I knew whose apartment was always spotless, always so tidy and well-organized: "Each time you're in a room, clean something". They were right. It really works. And it will work for our codebase too. Every time a change is made to a portion of the codebase, a change should be made to make it better, make it easier to work with, write a test, fix some documentation.

### Code Documentation

The hardest code to work with is the code that you have to read through in its entirety in order to use. Long gone are the days when one developer could keep the mental model for the CPU, the BIOS, the OS, the language, the libraries and the application itself all in their head. We depend on abstractions to get things done. Abstractions are built on contracts: if I add two and two I get four, if I divide by zero an exception is thrown, if I call `System.out.println("foo")` "foo" will be printed on the screen.

One of the ways to communicate these contracts is documentation. I don't mean one-line comments mixed in with the code, I mean documentation like that generated by [JavaDoc][javadoc]. JavaDoc is owes a lot to Donald Knuth's idea of [Literate Programming][literate] where the documentation and the code is written side-by-side in the same file[^1] and therefore maintained by the same people at, hopefully, the same time.

Generating this documentation and making it available to the developers can give people an overview of what each component is intended to do, without having to parse the code themselves. It can call out gotchas or edge cases that might not be so obvious in the code itself. And not only does code documentation serve the person calling some piece of code, but it also documents the contract of the code so that people don't accidentally change something that a consumer might be depending on.

### Automated Testing

Along with code documentation, automated tests provide valuable insight into the contracts that code adheres to. And automated tests go one step beyond most code documentation in that automated tests *verify* that code adheres to the contracts they specify.

Most people claim that automated tests don't find new bugs, they "just" prevent regressions. That is only partially true. Unit tests, most integration tests and even many automated end-to-end tests don't find new bugs. But stress testing does. Fuzz testing finds new bugs. Model-based testing finds new bugs. And all of these kinds of tests are dependent on having a system that is automatable. Model based testing systems, such as the one described in [this research paper][paper], have found bugs that took over 20 very specific steps to reproduce a crash in Windows Notepad. I have used such a system that I designed and built to verify that no path a user could take from launch to a depth of 10 steps had a crashing bug (and yes, it found bugs). I used a similar system to find crashing bugs based on user input (my favorite was "doofy!"). And automated testing covering unit tests, integration tests and even end-to-end tests are a *prerequisite* for this level of testing.

* All tests, static or dynamic, run as part of the build
* Prevent checkin or merge on failing tests
* Drive logic to where it is testable
* Invest in code quality verification tools
    * Code coverage
    * Cyclomatic complexity
    * Method length (LOC)
    * Class length (LOC)

## Not Making Things Worse

### Code Reviews

Not everyone is designed to do code reviews. Some people do best when working in their editor or IDE with their toolchain rather than visually scanning code. How do we enable these people to do their best work? We can provide tools for buddy builds. (Git is really good for this.) We can offer pair programming as an option. Pair programming has been described as "real-time code review". I think of it like this ...

I once had a manager whose style we described as "bring me a rock". If you asked him what kind of rock, he would simply say something along the lines of "it's not important" or "I don't have time" ... "just bring me a rock". Of course, when you found (or worse built) a rock and brought it to him, he would invariably say "that's not what I meant" and the cycle would begin again. It was a huge waste of everyone's time. Code reviews are essentially the "bring me a rock" style of management converted into writing code.

We fix this by pushing the feedback upstream. We have code review checkpoints. We do pair programming. We have design reviews. We do as much as possible to make sure that writing code is a *social* activity (*a la* GitHub), that nobody is allowed to run off into a cave and code on their own for more than X amount of time.

Code reviews can be and are useful. They are one of the nets that catch things before code gets committed or merged. As the saying goes, it can't be managed if it can't be measured. So let's build a review review system. Let's go through each piece of code review feedback and give it a score:

1. Style/Syntax
1. Logic
1. Design

We want to be catching 2s and 3s and not 1s. Catching 3s is not necessarily a bad thing, it just means that the design shifted during coding and probably for good reason! Senior developers should go through all code review feedback and determine what can be done to ensure that particular piece of feedback never needs to be given again. This can be achieved through:

* Tools
* Tests
* Best practices documentation (which should probably eventually feed into tools or tests)

### Design Reviews

**Every** work item of a determined level of complexity or above should be design reviewed. We can drive this into the Scrum process. If a team is using beans (1, 2, 3, 5 and 8 for example) maybe 5s and above need to be design reviewed. Senior developers should become the gate keepers of good design.

[^1]: Literate programming goes much further than JavaDoc does though. In literate programming systems, the documentation essentially forms the flow control of the program whereas JavaDoc simply documents the code without interfering with its structure.

[javadoc]: http://en.wikipedia.org/wiki/Javadoc
[literate]: http://en.wikipedia.org/wiki/Literate_programming
[paper]: http://repositorio-aberto.up.pt/bitstream/10216/7068/2/11242.pdf
[pit-of-success]: http://www.codinghorror.com/blog/2007/08/falling-into-the-pit-of-success.html


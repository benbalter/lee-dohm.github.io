---
layout: post
title: Tech Debt Thoughts
tags: Coder Tester
---

Often when people talk about quality practices, they get bogged down by trying to find the right balance between "getting work done" and "going slow enough to ensure quality". The quality of code does not always have to be in inverse proportion to the speed at which it is written. This document will describe practices that enhance the quality of code that gets produced while allowing engineers to innovate as fast as reasonably possible.

== Tenets ==

* Create tools to educate, encourage and, if necessary, enforce quality practices
* Design systems to create a [http://www.codinghorror.com/blog/2007/08/falling-into-the-pit-of-success.html Pit of Success]
* Understand that different people work differently and create systems that enable people to provide value according to their strengths rather than focusing on their weaknesses
* Only add process if it is absolutely necessary
* When process is necessary, keep it as lightweight as possible

== Builders Gonna Build ==

So get out of the way of people who want to build by adding automated quality checks and tools to the developer's process:

* Edit - Style checks, Static tests, Auto-compile, Tools to make writing code documentation easier
* Build - Style checks, static tests, documentation generation, unit tests
* Code Review - Create a tool to add results of style checks, static tests, unit tests to the code review description
* Commit - Prevent direct commit to mainline, all work is done on feature branches
* Merge - Prevent merging to mainline if tests or tools fail

There are multiple layers here:

# Tools that educate and inform as the engineer writes the code
# Tools that encourage better practices throughout the process
# Tools that enforce a minimum standard before sharing code with other developers

== Path to Green ==

=== Clean Up As You Go ===

When I first moved out, like most American young adults, I had a problem with keeping my place clean. Then I got some great advice from people I knew whose apartment was always spotless, always so tidy and well-organized: "Each time you're in a room, clean something". They were right. It really works. And it will work for our codebase too. Every time a change is made to a portion of the codebase, a change should be made to make it better, make it easier to work with, write a test, fix some documentation.

=== Code Documentation ===

The hardest code to work with is the code that you have to read through in its entirety in order to use. Long gone are the days when one developer could keep the mental model for the CPU, the BIOS, the OS, the language, the libraries and the application itself all in their head. We depend on abstractions to get things done. Abstractions are built on contracts: if I add two and two I get four, if I divide by zero an exception is thrown, if I call <code>System.out.println("foo")</code> "foo" will be printed on the screen. 

One of the ways to communicate these contracts is documentation. I don't mean one-line comments mixed in with the code, I mean documentation like that generated by [http://en.wikipedia.org/wiki/Javadoc JavaDoc]. JavaDoc is based off of a loose interpretation of Donald Knuth's idea of [http://en.wikipedia.org/wiki/Literate_programming Literate Programming] where the documentation and the code is written side-by-side in the same file and therefore maintained by the same people at, hopefully, the same time.

Generating this documentation and making it available to the developers can give people an overview of what each component is intended to do, without having to parse the code themselves. It can call out gotchas or edge cases that might not be so obvious in the code itself. And not only does code documentation serve the person calling some piece of code, but it also documents the contract of the code so that people don't accidentally change something that a consumer might be depending on.

=== Automated Testing ===

Along with code documentation, automated tests provide valuable insight into the contracts that code adheres to. And automated tests go one step beyond most code documentation in that automated tests ''verify'' that code adheres to the contracts they specify.

Most people claim that automated tests don't find new bugs, they "just" prevent regressions. That is only partially true. Unit tests, most integration tests and even many automated end-to-end tests don't find new bugs. But stress testing does. Fuzz testing finds new bugs. Model-based testing finds new bugs. And all of these kinds of tests are dependent on having a system that is automatable. Model based testing systems, such as the one described in [http://repositorio-aberto.up.pt/bitstream/10216/7068/2/11242.pdf this research paper], have found bugs that took over 20 very specific steps to reproduce a crash in Windows Notepad. I have used such a system that I designed and built to verify that no path a user could take from launch to a depth of 10 steps had a crashing bug (and yes, it found bugs). I used a similar system to find crashing bugs based on user input (my favorite was "doofy!"). And automated testing covering unit tests, integration tests and even end-to-end tests are a ''prerequisite'' for this level of testing.

* All tests, static or dynamic, run as part of the build
* Prevent checkin or merge on failing tests
* Drive logic to where it is testable
* Invest in code quality verification tools
** Code coverage
** Cyclomatic complexity
** Method length (LOC)
** Class length (LOC)

== Not Making Things Worse ==

=== Code Reviews ===

Not everyone is designed to do code reviews. Some people do best when working in their editor or IDE with their toolchain rather than visually scanning code. How do we enable these people to do their best work? We can provide tools for buddy builds. (Git is really good for this.) We can offer pair programming as an option. Pair programming has been described as "real-time code review". I think of it like this ...

I once had a manager whose style we described as "bring me a rock". If you asked him what kind of rock, he would simply say something along the lines of "it's not important" or "I don't have time" ... "just bring me a rock". Of course, when you found (or worse built) a rock and brought it to him, he would invariably say "that's not what I meant" and the cycle would begin again. It was a huge waste of everyone's time. Code reviews are essentially the "bring me a rock" style of management converted into writing code.

We fix this by pushing the feedback upstream. We have code review checkpoints. We do pair programming. We have design reviews. We do as much as possible to make sure that writing code is a ''social'' activity (''a la'' GitHub), that nobody is allowed to run off into a cave and code on their own for more than X amount of time.

Code reviews ''can'' be useful. They are one of the nets that catch things before code gets committed or merged. As the saying goes, it can't be managed if it can't be measured. So let's build a review review system. Let's go through each piece of code review feedback and give it a score:

# Style/Syntax
# Logic
# Design

We want to be catching 2s and 3s and not 1s. Catching 3s is not necessarily a bad thing, it just means that the design shifted during coding and probably for good reason! Senior developers should go through all code review feedback and determine what can be done to ensure that particular piece of feedback never needs to be given again. This can be achieved through:

* Tools
* Tests
* Best practices documentation (which should probably eventually feed into tools or tests)

=== Design Reviews ===

'''Every''' work item of a determined level of complexity or above should be design reviewed. We can drive this into the Scrum process. If a team is using beans (1, 2, 3, 5 and 8 for example) maybe 5s and above need to be design reviewed. Senior developers should become the gate keepers of good design.


